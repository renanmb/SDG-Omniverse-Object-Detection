{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554444a8",
   "metadata": {},
   "source": [
    "## Finetune pretrained model with your dataset "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44a814e-55b2-47e3-b353-e9a9de1c3a3e",
   "metadata": {},
   "source": [
    "Synthetic data is agnostic to the training framework, we are showing one way in which it can be used with Torchvision. However, when you create your own custom dataset you can plug that data into your own training workflow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90ca59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision import transforms as T\n",
    "import json\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82beca-cbb0-4901-a4a2-1345856b8e9a",
   "metadata": {},
   "source": [
    "We start by defining the epochs and classes for our training script. We have 10 total fruits we wish to identify in the images and we will start with running for 15 epochs. We can come back and adjust the epochs as we iterate on our training script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ece17c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 15\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fa7120-14b4-48aa-9574-caccda827569",
   "metadata": {},
   "source": [
    "We can navigate to our data that we generated by opening up a terminal in our Jupyter window. Press the \"+\" button in the top left to access a new terminal. Our data generation script defaults to `/dli/task/data/fruit_data_$DATE`. For now we have the data directory set to an example dataset you may choose to use. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a21b0216",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/dli/task/data/fruit_data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3cf09e-0ac8-4cba-8d7e-9a0a4e47c0f7",
   "metadata": {},
   "source": [
    "Next, we define our output dirtectory which is where we will save our PyTorch model. We also have an example model saved to `/dli/task/data/model.pth`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a8ab0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"/dli/task/model.pth\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5395cb-2787-4e58-8174-de190ef8c877",
   "metadata": {},
   "source": [
    "In our system today we are using a single A10 GPU. This gives us a powerful compute engine for training and state of the art tech for our graphics applications as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e52946a-6730-473e-aee0-b3be4f56137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01af1ac6-9124-4d4d-9a0b-12c5772a2d76",
   "metadata": {},
   "source": [
    "We are defining our device for the training to make sure we are able to use the A10 we have available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ed0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5697fc",
   "metadata": {},
   "source": [
    "Below we will define our `FruitDataset` class and the dataloader for the training. We have added comments throughout the code to explain each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f26dcb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FruitDataset(torch.utils.data.Dataset):\n",
    "    # This function is run once when instantiating the Dataset object\n",
    "    def __init__(self, root, transforms):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # In the first portion of this code we are taking our single dataset folder \n",
    "        # and splitting it into three folders based on the file types.\n",
    "        # This is just a preprocessing step.\n",
    "        list_ = os.listdir(root)\n",
    "        for file_ in list_:\n",
    "            name, ext = os.path.splitext(file_)\n",
    "            ext = ext[1:]\n",
    "            if ext == '':\n",
    "                continue\n",
    "\n",
    "            if os.path.exists(root+ '/' + ext):\n",
    "                shutil.move(root+'/'+file_, root+'/'+ext+'/'+file_)\n",
    "\n",
    "            else:\n",
    "                os.makedirs(root+'/'+ext)\n",
    "                shutil.move(root+'/'+file_, root+'/'+ext+'/'+file_)\n",
    "\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"png\"))))\n",
    "        self.label = list(sorted(os.listdir(os.path.join(root, \"json\"))))\n",
    "        self.box = list(sorted(os.listdir(os.path.join(root, \"npy\"))))\n",
    "        # We have our three attributes with the img, label, and box data\n",
    "\n",
    "    # Loads and returns a sample from the dataset at the given index idx\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"png\", self.imgs[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        label_path = os.path.join(self.root, \"json\", self.label[idx])\n",
    "\n",
    "        with open(os.path.join('root', label_path), \"r\") as json_data:\n",
    "            json_labels = json.load(json_data)\n",
    "        \n",
    "        box_path = os.path.join(self.root, \"npy\", self.box[idx])\n",
    "        dat = np.load(str(box_path))   \n",
    "\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        for i in dat:\n",
    "            obj_val = i[0]\n",
    "            xmin = torch.as_tensor(np.min(i[1]), dtype=torch.float32)\n",
    "            xmax = torch.as_tensor(np.max(i[3]), dtype=torch.float32)\n",
    "            ymin = torch.as_tensor(np.min(i[2]), dtype=torch.float32)\n",
    "            ymax = torch.as_tensor(np.max(i[4]), dtype=torch.float32)\n",
    "            if (ymax > ymin) & (xmax > xmin):\n",
    "                boxes.append([xmin, ymin, xmax, ymax])\n",
    "                area = (xmax - xmin) * (ymax - ymin)\n",
    "            labels += [json_labels.get(str(obj_val)).get('class')]\n",
    "\n",
    "        label_dict = {}\n",
    "\n",
    "        # Labels for the dataset\n",
    "        static_labels = {\n",
    "            'apple' : 0,\n",
    "            'avocado' : 1,\n",
    "            'kiwi' : 2,\n",
    "            'lime' : 3,\n",
    "            'lychee' : 4,\n",
    "            'pomegranate' : 5,\n",
    "            'onion' : 6,\n",
    "            'strawberry' : 7,\n",
    "            'lemon' : 8,\n",
    "            'orange' : 9\n",
    "        }\n",
    "\n",
    "        labels_out = []\n",
    "        # Transforming the input labels into a static label dictionary to use\n",
    "        for i in range(len(labels)):\n",
    "            label_dict[i] = labels[i]\n",
    "\n",
    "        for i in label_dict:\n",
    "            fruit = label_dict[i]\n",
    "            final_fruit_label = static_labels[fruit]\n",
    "            labels_out += [final_fruit_label]\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        target[\"labels\"] = torch.as_tensor(labels_out, dtype=torch.int64)\n",
    "        target[\"image_id\"] = torch.tensor([idx]) \n",
    "        target[\"area\"] = area\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img= self.transforms(img)\n",
    "        return img, target\n",
    "\n",
    "    # Finally we have a function for the number of samples in our dataset\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03126d04-2a5e-48c3-85ad-4412e7a2c1b9",
   "metadata": {},
   "source": [
    "Next we are defining our function for the feature and label transformations we wish to preform. We are converting to Tenso and also converting the dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925e7dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(T.PILToTensor())\n",
    "    transforms.append(T.ConvertImageDtype(torch.float))\n",
    "    return T.Compose(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b8b5f-c0c2-4cc0-985b-c37b45b25aa1",
   "metadata": {},
   "source": [
    "Create a function to collate our samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a11f4381",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7cc8acb-257c-4d3f-ab91-c5a5849c67f8",
   "metadata": {},
   "source": [
    "Next we go through the process of actually creating our model. We are using the `fasterrcnn_resnet50` pretrained model from torchvision in this example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b17f563",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(num_classes): \n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights='DEFAULT')\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes) \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa97d0c-a404-47e6-9493-c5d6327e34c9",
   "metadata": {},
   "source": [
    "At this point we are ready to create our dataset by using our custom `FruitDataset` class and our synthetic  data. This is then passed into our DataLoader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9f447e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = FruitDataset(data_dir, get_transform(train=True))\n",
    "data_loader = torch.utils.data.DataLoader(\n",
    "dataset, batch_size=16, shuffle=True, collate_fn= collate_fn) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36d7436-f2eb-4d06-8cfc-63a8c210a094",
   "metadata": {},
   "source": [
    "Next we create our model with the 10 classes we have of fruit and transfer to the GPU for training. We use torch SDG for stochastic gradient descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e08427",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(num_classes)\n",
    "model.to(device)\n",
    "    \n",
    "params = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.SGD(params, lr=0.001)\n",
    "len_dataloader = len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a1ed0f-72a7-4312-b17a-565f6f0efc85",
   "metadata": {},
   "source": [
    "In our final section we actually train our model. We keep track of our loss and print out as we train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e828acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    i = 0    \n",
    "    for imgs, annotations in data_loader:\n",
    "        i += 1\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        annotations = [{k: v.to(device) for k, v in t.items()} for t in annotations]\n",
    "        loss_dict = model(imgs, annotations)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Iteration: {i}/{len_dataloader}, Loss: {losses}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2446ad38-4df4-4f87-b132-ff140a67ba7f",
   "metadata": {},
   "source": [
    "Our final step is to save the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a1e0a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900be67-11b9-4cee-bf23-9ab85174d7e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
