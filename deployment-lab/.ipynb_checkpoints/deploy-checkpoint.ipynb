{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Model Serving\n",
    "Centralized model serving can be a huge design win for your business products and/or applications. Hosting models in a central location reduces memory usage and can be designed to reduce inter-device communication.\n",
    "\n",
    "This example will use [Nvidia's Triton Inference Server](https://github.com/triton-inference-server/server) to serve the model exported in the previous section.\n",
    "\n",
    "But first lets start by bringing up the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!mkdir /opt/model_repository/ # create the folder for the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Triton Inference Server\n",
    "import subprocess\n",
    "server = subprocess.Popen([\"tritonserver\", \"--model-repository=/opt/model_repository/\", \"--model-control-mode=poll\", \"--http-port=8988\"])\n",
    "server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of Triton is the [ability to have it \"poll\" a model repository](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_management.md#model-control-mode-poll) to see if a change has occurred. So all that needs to be done is copy the model into the `model_repository` directory. You can read more on the specifics [here](https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_repository.md#repository-layout)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /opt/model_repository/our_new_model/ # create the folder with the model name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /opt/model_repository/our_new_model/1/ # create the folder with the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we copy our model into the repository. We have our own onnx model copying over, if you would like to use yours, uncomment asnd run the other line instead. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /dli/task/data/model.onnx /opt/model_repository/our_new_model/1/model.onnx # move the file to the directory\n",
    "#!cp /dli/task/data/custom_onnx/model.onnx /opt/model_repository/our_new_model/1/model.onnx # Delete above line and run this is you want to use you ONNX model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that the model is in Triton, it has automatically created a model config and loaded it. Let's query it to see more about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import tritonclient.grpc as grpcclient\n",
    "\n",
    "inference_server_url = \"localhost:8001\"\n",
    "triton_client = grpcclient.InferenceServerClient(url=inference_server_url)\n",
    "\n",
    "# find out info about model\n",
    "model_name = \"our_new_model\"\n",
    "triton_client.get_model_config(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "You can also create a custom config to control other parameters like batch size or maximum number of requests.\n",
    "\n",
    "However now we are going to do our inference with the model!\n",
    "\n",
    "You can see in the config above we have the input and output names of the model. Let's use this information to do inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tritonclient.utils import triton_to_np_dtype\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# load image data\n",
    "target_width, target_height = 1024, 1024\n",
    "image_bgr = cv2.imread(\"sample_image.png\")\n",
    "image_bgr = cv2.resize(image_bgr, (target_width, target_height))\n",
    "image_rgb = cv2.cvtColor(image_bgr, cv2.COLOR_BGR2RGB)\n",
    "image = np.float32(image_rgb)\n",
    "\n",
    "# preprocessing\n",
    "image = image/255\n",
    "image = np.moveaxis(image, -1, 0)  # HWC to CHW\n",
    "    \n",
    "image = image[np.newaxis, :] # add batch dimension\n",
    "image = np.float32(image)\n",
    "\n",
    "plt.imshow(image_rgb)\n",
    "\n",
    "# create input\n",
    "input_name = \"input\"\n",
    "inputs = [grpcclient.InferInput(input_name, image.shape, \"FP32\")]\n",
    "inputs[0].set_data_from_numpy(image)\n",
    "\n",
    "output_names = [\"boxes\", \"labels\", \"scores\"]\n",
    "outputs = [grpcclient.InferRequestedOutput(n) for n in output_names]\n",
    "\n",
    "\n",
    "results = triton_client.infer(model_name, inputs, outputs=outputs)\n",
    "\n",
    "boxes, labels, scores = [results.as_numpy(o) for o in output_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# annotate\n",
    "annotated_image = image_bgr.copy()\n",
    "            \n",
    "if boxes.size > 0:  # ensure something is found\n",
    "    for box, lab, scr in zip(boxes, labels, scores):\n",
    "\n",
    "        if scr > 0.2:\n",
    "            box_top_left = int(box[0]), int(box[1])\n",
    "            box_bottom_right = int(box[2]), int(box[3])\n",
    "            text_origin = int(box[0]), int(box[3])\n",
    "\n",
    "            border_color = (50, 0, 100)\n",
    "            text_color = (255, 255, 255)\n",
    "\n",
    "            font_scale = 0.9\n",
    "            thickness = 1\n",
    "\n",
    "            # bounding box2\n",
    "            cv2.rectangle(annotated_image, box_top_left, box_bottom_right, border_color, thickness=5,\n",
    "                          lineType=cv2.LINE_8)\n",
    "        \n",
    "plt.imshow(cv2.cvtColor(annotated_image, cv2.COLOR_BGR2RGB))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congrats! You have now done inference with a model ready for a production deployment!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
